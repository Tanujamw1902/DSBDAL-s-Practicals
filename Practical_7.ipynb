{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddfb0b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tanuja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tanuja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tanuja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Tanuja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sentence Tokenization: \n",
      " ['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']\n",
      "\n",
      "Word Tokeniztion: \n",
      " ['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n",
      "\n",
      "POS Tagging: \n",
      " [('Tokenization', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('first', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('text', 'JJ'), ('analytics', 'NNS'), ('.', '.'), ('The', 'DT'), ('process', 'NN'), ('of', 'IN'), ('breaking', 'VBG'), ('down', 'RP'), ('a', 'DT'), ('text', 'NN'), ('paragraph', 'NN'), ('into', 'IN'), ('smaller', 'JJR'), ('chunks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('words', 'NNS'), ('or', 'CC'), ('sentences', 'NNS'), ('is', 'VBZ'), ('called', 'VBN'), ('Tokenization', 'NN'), ('.', '.')]\n",
      "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
      "Filtered Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n",
      "Stemming for  wait :  wait\n",
      "Stemming for  waiting :  wait\n",
      "Stemming for  waited :  wait\n",
      "Stemming for  waits :  wait\n",
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n",
      "----------------Term Frequency----------------------\n",
      "     Sun  planet  largest   Mars  fourth     is  Jupiter   from   the\n",
      "0  0.000   0.200      0.2  0.000   0.000  0.200      0.2  0.000  0.20\n",
      "1  0.125   0.125      0.0  0.125   0.125  0.125      0.0  0.125  0.25\n",
      "----------------Inverse Document Frequency----------------------\n",
      "{'Sun': 0.6931471805599453, 'planet': 0.0, 'largest': 0.6931471805599453, 'Mars': 0.6931471805599453, 'fourth': 0.6931471805599453, 'is': 0.0, 'Jupiter': 0.6931471805599453, 'from': 0.6931471805599453, 'the': 0.0}\n",
      "------------------- TF-IDF--------------------------------------\n",
      "        Sun  planet   largest      Mars    fourth   is   Jupiter      from  \\\n",
      "0  0.000000     0.0  0.138629  0.000000  0.000000  0.0  0.138629  0.000000   \n",
      "1  0.086643     0.0  0.000000  0.086643  0.086643  0.0  0.000000  0.086643   \n",
      "\n",
      "   the  \n",
      "0  0.0  \n",
      "1  0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sentence Tokenization\n",
    "text = \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\"\n",
    "tokenized_text = nltk.sent_tokenize(text)\n",
    "print(\"\\n Sentence Tokenization: \\n\", tokenized_text)\n",
    "\n",
    "# Word Tokenization\n",
    "tokenized_word = nltk.word_tokenize(text)\n",
    "print('\\nWord Tokeniztion: \\n', tokenized_word)\n",
    "\n",
    "# POS Tagging\n",
    "tagged_text = nltk.pos_tag(tokenized_word)\n",
    "print(\"\\nPOS Tagging: \\n\", tagged_text)\n",
    "\n",
    "# Stop words removal\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "text = \"How to remove stop words with NLTK library in Python?\"\n",
    "text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "tokens = nltk.word_tokenize(text.lower())\n",
    "filtered_text = [w for w in tokens if w not in stop_words]\n",
    "print (\"Tokenized Sentence:\", tokens)\n",
    "print (\"Filtered Sentence:\", filtered_text)\n",
    "\n",
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "e_words = [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "ps = PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord = ps.stem(w)\n",
    "    print('Stemming for ', w, ': ', rootWord)\n",
    "\n",
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))\n",
    "\n",
    "# Algorithm for Create representation of document by calculating TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Step 1: Import the necessary libraries.\n",
    "documentA = 'Jupiter is the largest planet'\n",
    "documentB = 'Mars is the fourth planet from the Sun'\n",
    "\n",
    "# Step 2: Initialize the Documents.\n",
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')\n",
    "\n",
    "# Step 3: Create BagofWords (BoW) for Document A and B. word tokenization\n",
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
    "\n",
    "# Step 4: Create Collection of Unique words from Document A and B.\n",
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1 #How many times each word is repeated\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1\n",
    "\n",
    "# Step 5: Compute the term frequency for each of our documents.\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
    "\n",
    "# Step 6: Compute the term Inverse Document Frequency.\n",
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:idfDict[word] += 1\n",
    "    for word, val in idfDict.items():\n",
    "        if val > 0: idfDict[word] = math.log(N / float(val))\n",
    "        else: idfDict[word] = 0\n",
    "    return idfDict\n",
    "\n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
    "\n",
    "# Step 7: Compute the term TF/IDF for all words.\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "\n",
    "print('----------------Term Frequency----------------------')\n",
    "df = pd.DataFrame([tfA, tfB])\n",
    "print(df)\n",
    "\n",
    "print('----------------Inverse Document Frequency----------------------')\n",
    "print(idfs)\n",
    "\n",
    "print('------------------- TF-IDF--------------------------------------')\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff078e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
